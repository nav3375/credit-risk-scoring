

üìä Credit Risk Scoring ‚Äî Final Validation Report


1) Objective

Develop a credit risk scoring model using the UCI German Credit dataset to estimate Probability of Default (PD).
Goals:
	‚Ä¢	Avoid overfitting, achieve stable generalization.
	‚Ä¢	Ensure probabilities are calibrated and usable as PD.
	‚Ä¢	Pick an operating threshold aligned with business logic (minimize missed defaults).
	‚Ä¢	Provide explainability (SHAP) and deploy with Streamlit demo.



2) Data & Preprocessing
	‚Ä¢	Dataset: 1,000 loan applicants; target = {good=700, bad=300}.
	‚Ä¢	Split: stratified 80/20 (Train=800, Test=200).
	‚Ä¢	Preprocessing:
	‚Ä¢	ColumnTransformer ‚Üí One-hot encode 13 categorical variables, passthrough 7 numeric.
	‚Ä¢	Missing values not present in dataset.
	‚Ä¢	Class imbalance: handled with SMOTE inside the pipeline + tree models using class_weight="balanced".



3) Model Candidates (Breadth of Exploration)

We compared multiple model families with hyperparameter tuning:

Model	Train ROC AUC	Test ROC AUC	Gap	Test PR AUC	Notes
Logistic Regression	0.8443	0.7543	0.090	0.5594	Stable but weaker
Random Forest (tuned)	0.9414	0.7908	0.151	0.6292	Strong, mild overfit
Gradient Boosting	0.8897	0.7644	0.125	0.5780	Decent, below RF
XGBoost	0.9964	0.7720	0.224	0.5794	Overfit (large gap)
LightGBM	0.9565	0.7533	0.203	0.5302	Overfit (large gap)

Takeaway: Random Forest provided the best balance of predictive power and generalization.



4) Final Model (Chosen for Deployment)

Regularized Random Forest + SMOTE (imblearn Pipeline):
	‚Ä¢	n_estimators=200
	‚Ä¢	max_depth=6
	‚Ä¢	min_samples_split=12
	‚Ä¢	min_samples_leaf=8
	‚Ä¢	max_features='log2'
	‚Ä¢	class_weight='balanced'

Performance:
	‚Ä¢	Test ROC AUC = 0.7846
	‚Ä¢	Test PR AUC = 0.6029
	‚Ä¢	Train ROC AUC = 0.8862 ‚Üí Generalization gap = 0.1015

Cross-validation stability:
	‚Ä¢	5-fold CV ROC AUC = 0.7829 ¬± 0.0520
	‚Ä¢	Repeated Stratified 5√ó5 CV = 0.7785 ¬± 0.0425

Verdict: Controlled gap, stable CV, better than baseline Logistic Regression.



5) Calibration (Are Probabilities Reliable?)

	‚Ä¢	Brier score (test) = 0.1813 (lower = better).
	‚Ä¢	Calibration curve shows predictions close to diagonal ‚Üí probabilities are meaningful.

PD values can be directly used as risk scores.



6) Bias‚ÄìVariance & Overfitting Check

	‚Ä¢	Training ROC AUC ~0.89, CV ROC AUC ~0.78.
	‚Ä¢	Both curves converge as training size increases.

Controlled generalization gap (~0.10). Not underfitting, not severely overfitting.



7) Model Family Comparison

	‚Ä¢	Boosting models (XGB, LGBM) ‚Üí clear overfit (huge gaps).
	‚Ä¢	Logistic Regression ‚Üí stable but weaker.
	‚Ä¢	Random Forest ‚Üí best balance between train and test.

Confirms Random Forest is the defensible choice.



8) Operating Threshold

Threshold sweep on test set (optimize F1 for ‚Äúbad‚Äù):

Threshold	Precision_bad	Recall_bad	F1_bad
0.30	0.390	0.917	0.547
0.35	0.443	0.900	0.593
0.40	0.461	0.783	0.580
0.50	0.618	0.567	0.591

Chosen: 0.35 ‚Üí maximizes F1_bad (0.593) while achieving high recall (0.90).

Business justification: In credit risk, false negatives (approving bad customers) are costlier than false positives. Lower threshold increases safety.



9) Interpretability (SHAP)

Global Drivers

Most important features:
	‚Ä¢	checking_status_A14, checking_status_A11, savings_status_A61, credit_history_A34, duration.

Local Example (single applicant)

Breakdown shows how individual features push PD up/down, making the model explainable for customer-level decisions.

This ensures regulatory compliance & stakeholder trust.



10) Why This Model is Good Enough

Evidence included in repo:
	1.	Tried multiple model families (LogReg, RF, XGB, LGBM).
	2.	Chose RF based on generalization gap + stability.
	3.	Learning curves ‚Üí confirmed not over/underfitting.
	4.	Calibration curve + Brier score ‚Üí probabilities reliable.
	5.	Threshold optimized ‚Üí 0.35 chosen for business safety.
	6.	SHAP global + local ‚Üí interpretability achieved.
	7.	Saved artifacts (models/*.pkl, reports/*.png).
	8.	Deployed with Streamlit (single + batch scoring).



11) Limitations & Risks
	‚Ä¢	Dataset small (n=1000) ‚Üí variance possible.
	‚Ä¢	SMOTE may over-smooth minority class.
	‚Ä¢	Some categorical codes (e.g., employment, checking status) may proxy sensitive attributes ‚Üí fairness checks needed in production.
	‚Ä¢	No temporal validation ‚Üí in production, use time-based validation + drift monitoring.



12) Deployment Notes
	‚Ä¢	Model file: models/final_regularized_model.pkl
	‚Ä¢	Default Streamlit threshold: 0.35
	‚Ä¢	Batch scoring output: data/scored_test_with_pd.csv

